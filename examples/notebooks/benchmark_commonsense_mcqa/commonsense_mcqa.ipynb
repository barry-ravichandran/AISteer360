{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3290202-4183-4f46-81d3-a817d66d4f2b",
   "metadata": {},
   "source": [
    "# Commonsense MCQA\n",
    "\n",
    "Multiple choice question answering is a common format for evaluating a model's reasoning ability. This notebook benchmarks few-shot prompting against a LoRA adapter (trained with DPO) on the [CommonsenseQA](https://huggingface.co/datasets/tau/commonsense_qa) dataset. We sweep over the number of (positive) few-shot examples and study how accuracy scales relative to the fine-tuned baseline across two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c43de8",
   "metadata": {},
   "source": [
    "### Runtime Estimate\n",
    "\n",
    "> **Estimated Time:** 2-3 hours (fine-tuning two models on ~39k preference pairs)  \n",
    "> **Device:** NVIDIA H100 GPU (80GB VRAM)\n",
    "\n",
    "Times are approximate and vary based on dataset size, number of sweeps, and model configuration. Adjust parameters in the cells below to modify runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb9d35",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pi8wn8f6sch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import PeftType\n",
    "\n",
    "from aisteer360.algorithms.input_control.few_shot.control import FewShot\n",
    "from aisteer360.algorithms.core.specs import ControlSpec\n",
    "from aisteer360.algorithms.structural_control.wrappers.trl.dpotrainer.control import DPO\n",
    "from aisteer360.evaluation.use_cases.commonsense_mcqa.use_case import CommonsenseMCQA\n",
    "from aisteer360.evaluation.metrics.custom.commonsense_mcqa.mcqa_accuracy import MCQAAccuracy\n",
    "from aisteer360.evaluation.metrics.custom.commonsense_mcqa.mcqa_positional_bias import MCQAPositionalBias\n",
    "from aisteer360.evaluation.benchmark import Benchmark\n",
    "from aisteer360.evaluation.utils.data_utils import flatten_profiles, get_param_values, summarize_by_config\n",
    "from aisteer360.evaluation.utils.viz_utils import plot_sensitivity, plot_tradeoff\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "MODELS = [\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "]\n",
    "\n",
    "NOTEBOOK_DIR = Path(__file__).parent if \"__file__\" in dir() else Path.cwd() / \"examples/notebooks/benchmark_commonsense_mcqa\"\n",
    "FIGURE_DIR = NOTEBOOK_DIR / \"figures\"\n",
    "FIGURE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "LETTERS = \"ABCDE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-md",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We load the [CommonsenseQA](https://huggingface.co/datasets/tau/commonsense_qa) dataset from Hugging Face. The `validation` split is used for evaluation and the `train` split is used for steering (few-shot example pools and DPO training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-data-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation split: 1221 questions\n",
      "Steering split: 9741 questions\n"
     ]
    }
   ],
   "source": [
    "csqa = load_dataset(\"tau/commonsense_qa\")\n",
    "\n",
    "eval_split = csqa[\"validation\"]\n",
    "steer_split = csqa[\"train\"]\n",
    "\n",
    "print(f\"Evaluation split: {len(eval_split)} questions\")\n",
    "print(f\"Steering split: {len(steer_split)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd900f7e",
   "metadata": {},
   "source": [
    "The `CommonsenseMCQA` use case expects each evaluation record to contain the question text, the correct answer text, \n",
    "and the full list of choices (so that it can shuffle them across runs to measure positional bias). We build these records \n",
    "directly from the `validation` split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a81e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1221,\n",
       " {'id': '1afa02df02c908a558b4036e80242fac',\n",
       "  'question': 'A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?',\n",
       "  'answer': 'bank',\n",
       "  'choices': ['bank', 'library', 'department store', 'mall', 'new york']})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_records = []\n",
    "for row in eval_split:\n",
    "    correct_idx = LETTERS.index(row[\"answerKey\"])\n",
    "    choices = row[\"choices\"][\"text\"]\n",
    "    eval_records.append({\n",
    "        \"id\": row[\"id\"],\n",
    "        \"question\": row[\"question\"],\n",
    "        \"answer\": choices[correct_idx],\n",
    "        \"choices\": choices,\n",
    "    })\n",
    "\n",
    "len(eval_records), eval_records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b776eb",
   "metadata": {},
   "source": [
    "## Building the use case\n",
    "\n",
    "The use case of interest has already been constructed via the [use case](../../../docs/tutorials/add_new_use_case.md) \n",
    "tutorial and is available at `aisteer360/evaluation/use_cases/commonsense_mcqa/use_case.py`. We pass in `eval_records`\n",
    "as the evaluation data for the use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3f0683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense_mcqa = CommonsenseMCQA(\n",
    "    evaluation_data=eval_records,\n",
    "    evaluation_metrics=[MCQAAccuracy(), MCQAPositionalBias()],\n",
    "    num_samples=50,\n",
    "    num_shuffling_runs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135bf075",
   "metadata": {},
   "source": [
    "Two custom metrics have been created for the use case: \n",
    "- `MCQAAccuracy`: measures the accuracy statistics of each question (across trials)\n",
    "- `MCQAPositionalBias`: measures the positional bias (via deviation from the uniform distribution across runs)\n",
    "\n",
    "To facilitate computation of these statistics, the use case accepts a keyword argument `num_shuffling_runs` dictating \n",
    "how many times each question should be presented to the (steered) model under a randomized ordering of the choices. \n",
    "We restrict the number of evaluation datapoints to `num_samples=50` for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400cf920",
   "metadata": {},
   "source": [
    "## Preparing the steering data\n",
    "\n",
    "Both steering methods draw from the `train` (steer) split and share a common MCQA prompt format: a question with lettered choices, expecting a single letter response. We define this format once and reuse it for both the few-shot example pools and the DPO preference pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23ebdfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mcqa_prompt(question: str, choices: list[str]) -> str:\n",
    "    lines = [\"You will be given a multiple-choice question and asked to select from a set of choices.\"]\n",
    "    lines.append(f\"\\nQuestion: {question}\\n\")\n",
    "    for i, choice in enumerate(choices):\n",
    "        lines.append(f\"{LETTERS[i]}. {choice}\")\n",
    "    lines.append(\"\\nPlease only print the letter corresponding to your choice.\")\n",
    "    lines.append(\"\\nAnswer:\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b716a7c",
   "metadata": {},
   "source": [
    "### Few-shot example pools\n",
    "\n",
    "For FewShot, we build positive and negative example pools from the training split. Each example is a formatted MCQA prompt paired with a letter answer, matching the format the model will see at evaluation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c63701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot pools: 9741 positive, 9741 negative\n",
      "\n",
      "Example positive:\n",
      "  Prompt: You will be given a multiple-choice question and asked to select from a set of choices.\n",
      "\n",
      "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the schoo...\n",
      "  Answer: A\n"
     ]
    }
   ],
   "source": [
    "positive_pool = []\n",
    "negative_pool = []\n",
    "\n",
    "for row in steer_split:\n",
    "    choices = row[\"choices\"][\"text\"]\n",
    "    correct_idx = LETTERS.index(row[\"answerKey\"])\n",
    "    prompt = format_mcqa_prompt(row[\"question\"], choices)\n",
    "\n",
    "    wrong_indices = [i for i in range(len(choices)) if i != correct_idx]\n",
    "    positive_pool.append({\"prompt\": prompt, \"answer\": LETTERS[correct_idx]})\n",
    "    negative_pool.append({\"prompt\": prompt, \"answer\": LETTERS[wrong_indices[0]]})\n",
    "\n",
    "print(f\"Few-shot pools: {len(positive_pool)} positive, {len(negative_pool)} negative\")\n",
    "print(f\"\\nExample positive:\")\n",
    "print(f\"  Prompt: {positive_pool[0]['prompt'][:200]}...\")\n",
    "print(f\"  Answer: {positive_pool[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1471403f",
   "metadata": {},
   "source": [
    "### DPO preference pairs\n",
    "\n",
    "For DPO, we create preference pairs using the same prompt format. Each pair contrasts the correct letter against an incorrect one. To increase training diversity, we create up to four pairs per question by contrasting the correct answer against each wrong answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "866cc8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 38964 DPO preference pairs from 9741 questions\n",
      "\n",
      "Example pair:\n",
      "  Prompt: You will be given a multiple-choice question and asked to select from a set of choices.\n",
      "\n",
      "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the schoo...\n",
      "  Chosen: A\n",
      "  Rejected: B\n"
     ]
    }
   ],
   "source": [
    "dpo_pairs = []\n",
    "for row in steer_split:\n",
    "    choices = row[\"choices\"][\"text\"]\n",
    "    correct_idx = LETTERS.index(row[\"answerKey\"])\n",
    "    prompt = format_mcqa_prompt(row[\"question\"], choices)\n",
    "\n",
    "    wrong_indices = [i for i in range(len(choices)) if i != correct_idx]\n",
    "    for wrong_idx in wrong_indices[:4]:\n",
    "        dpo_pairs.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": LETTERS[correct_idx],\n",
    "            \"rejected\": LETTERS[wrong_idx],\n",
    "        })\n",
    "\n",
    "train_ds = Dataset.from_list(dpo_pairs)\n",
    "\n",
    "print(f\"Created {len(train_ds)} DPO preference pairs from {len(steer_split)} questions\")\n",
    "print(f\"\\nExample pair:\")\n",
    "print(f\"  Prompt: {train_ds[0]['prompt'][:200]}...\")\n",
    "print(f\"  Chosen: {train_ds[0]['chosen']}\")\n",
    "print(f\"  Rejected: {train_ds[0]['rejected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f80559-54d0-4740-a7c6-ca10f2d0c999",
   "metadata": {},
   "source": [
    "## Defining the controls\n",
    "\n",
    "### FewShot with ControlSpec\n",
    "\n",
    "One of the goals of the invesitgation in this notebook is to explore how the number of (in-context) examples impacts model behavior. We use the toolkit's `ControlSpec` class to sweep over different values of `k_positive`. We fix `k_negative=0` to isolate the effect of positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd33d9d-30a7-4715-8af6-04080d8e87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_spec = ControlSpec(\n",
    "    control_cls=FewShot,\n",
    "    params={\n",
    "        \"selector_name\": \"random\",\n",
    "        \"positive_example_pool\": positive_pool,\n",
    "        \"negative_example_pool\": negative_pool,\n",
    "        \"k_negative\": 0,\n",
    "    },\n",
    "    vars=[{\"k_positive\": k} for k in [1, 5, 10, 25, 50]],\n",
    "    name=\"FewShot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30194591-7892-496f-995e-59e3df656da1",
   "metadata": {},
   "source": [
    "### DPO with LoRA\n",
    "\n",
    "The DPO-LoRA control fine-tunes a LoRA adapter using the preference pairs (`dpo_pairs`) we created above. The two models have slightly different training requirements; so we create a convenience function that populates the controls as a function of configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dpo_factory_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "DPO_CONFIGS = {\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\": {\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"num_train_epochs\": 5,\n",
    "    },\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": {\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"num_train_epochs\": 3,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def create_dpo_control(model_name: str) -> DPO:\n",
    "    \"\"\"Create a DPO control with model-specific hyperparameters.\"\"\"\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    config = DPO_CONFIGS.get(model_name, DPO_CONFIGS[\"Qwen/Qwen2.5-0.5B-Instruct\"])\n",
    "\n",
    "    return DPO(\n",
    "        train_dataset=train_ds,\n",
    "\n",
    "        # DPO / TRL config\n",
    "        output_dir=NOTEBOOK_DIR / f\"trl_models/{short_name}-DPO-Lora-Steer\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=config[\"num_train_epochs\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        beta=0.1,\n",
    "        loss_type=\"sigmoid\",\n",
    "        max_length=512,\n",
    "        max_prompt_length=450,\n",
    "        disable_dropout=True,\n",
    "        logging_steps=200,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        seed=123,\n",
    "\n",
    "        # LoRA config\n",
    "        use_peft=True,\n",
    "        peft_type=PeftType.LORA,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        adapter_name=\"dpo\",\n",
    "        merge_lora_after_train=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7ca68-0e1b-4fde-a235-b08c2537bd84",
   "metadata": {},
   "source": [
    "## Running the benchmark\n",
    "\n",
    "The benchmark compares three steering approaches across multiple model sizes:\n",
    "- **baseline**: Unsteered model\n",
    "- **few_shot_sweep**: FewShot with varying `k_positive` (1, 5, 10, 25, 50)\n",
    "- **dpo_lora**: DPO-trained LoRA adapter\n",
    "\n",
    "We run with `num_trials=5` to capture statistical variability across generation runs (at the cost of slower execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0005b-c0b0-4879-acc3-241e3f014307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark for Qwen2.5-0.5B-Instruct\n",
      "Running pipeline: baseline...\n",
      "done.\n",
      "Running pipeline: few_shot_sweep...\n",
      "Running configuration 1...\n",
      "Running configuration 2...\n",
      "Running configuration 3...\n",
      "Running configuration 4...\n",
      "Running configuration 5...\n",
      "done.\n",
      "Running pipeline: dpo_lora...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38964/38964 [00:00<00:00, 52982.88 examples/s]\n",
      "Extracting prompt in train dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38964/38964 [00:01<00:00, 33927.36 examples/s]\n",
      "Applying chat template to train dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38964/38964 [00:00<00:00, 40298.68 examples/s]\n",
      "Tokenizing train dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38964/38964 [00:08<00:00, 4588.73 examples/s]\n",
      "Train dataset reference log probs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [21:54<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4405, 'grad_norm': 3.811967372894287, 'learning_rate': 4.918308702791461e-05, 'rewards/chosen': -0.37143298983573914, 'rewards/rejected': -1.545861840248108, 'rewards/accuracies': 0.7996875047683716, 'rewards/margins': 1.1744288206100464, 'logps/chosen': -35.37575912475586, 'logps/rejected': -48.15895462036133, 'logits/chosen': -2.738180637359619, 'logits/rejected': -2.7275230884552, 'epoch': 0.0821186614658181}\n",
      "{'loss': 0.3837, 'grad_norm': 7.952126979827881, 'learning_rate': 4.8362068965517246e-05, 'rewards/chosen': 0.06859302520751953, 'rewards/rejected': -1.583513617515564, 'rewards/accuracies': 0.8346874713897705, 'rewards/margins': 1.652106761932373, 'logps/chosen': -30.929655075073242, 'logps/rejected': -48.548828125, 'logits/chosen': -2.9601025581359863, 'logits/rejected': -2.9752004146575928, 'epoch': 0.1642373229316362}\n",
      "{'loss': 0.348, 'grad_norm': 7.980932235717773, 'learning_rate': 4.754105090311987e-05, 'rewards/chosen': 0.14456726610660553, 'rewards/rejected': -1.8365343809127808, 'rewards/accuracies': 0.8521875143051147, 'rewards/margins': 1.981101632118225, 'logps/chosen': -30.17273712158203, 'logps/rejected': -51.134830474853516, 'logits/chosen': -2.6266937255859375, 'logits/rejected': -2.6566290855407715, 'epoch': 0.24635598439745432}\n",
      "{'loss': 0.3266, 'grad_norm': 12.198899269104004, 'learning_rate': 4.67200328407225e-05, 'rewards/chosen': 0.3668653964996338, 'rewards/rejected': -1.8787016868591309, 'rewards/accuracies': 0.8553125262260437, 'rewards/margins': 2.2455670833587646, 'logps/chosen': -27.91363525390625, 'logps/rejected': -51.464717864990234, 'logits/chosen': -2.590886116027832, 'logits/rejected': -2.6335973739624023, 'epoch': 0.3284746458632724}\n",
      "{'loss': 0.3231, 'grad_norm': 7.032894611358643, 'learning_rate': 4.5899014778325125e-05, 'rewards/chosen': 0.2052580714225769, 'rewards/rejected': -2.1386947631835938, 'rewards/accuracies': 0.8678125143051147, 'rewards/margins': 2.3439526557922363, 'logps/chosen': -29.529855728149414, 'logps/rejected': -54.10764694213867, 'logits/chosen': -2.661221981048584, 'logits/rejected': -2.770111083984375, 'epoch': 0.41059330732909055}\n",
      "{'loss': 0.3285, 'grad_norm': 4.933930397033691, 'learning_rate': 4.507799671592775e-05, 'rewards/chosen': -0.33506977558135986, 'rewards/rejected': -2.593303918838501, 'rewards/accuracies': 0.8596875071525574, 'rewards/margins': 2.2582340240478516, 'logps/chosen': -34.99370574951172, 'logps/rejected': -58.66734313964844, 'logits/chosen': -2.9775431156158447, 'logits/rejected': -3.1301605701446533, 'epoch': 0.49271196879490864}\n",
      "{'loss': 0.3065, 'grad_norm': 3.397167444229126, 'learning_rate': 4.425697865353038e-05, 'rewards/chosen': -0.4392448365688324, 'rewards/rejected': -2.8525171279907227, 'rewards/accuracies': 0.8715624809265137, 'rewards/margins': 2.4132721424102783, 'logps/chosen': -36.02518844604492, 'logps/rejected': -61.133636474609375, 'logits/chosen': -2.6093251705169678, 'logits/rejected': -2.724609375, 'epoch': 0.5748306302607268}\n",
      "{'loss': 0.3005, 'grad_norm': 8.09684944152832, 'learning_rate': 4.3435960591133004e-05, 'rewards/chosen': -0.42275410890579224, 'rewards/rejected': -3.123645544052124, 'rewards/accuracies': 0.8815624713897705, 'rewards/margins': 2.7008914947509766, 'logps/chosen': -35.810333251953125, 'logps/rejected': -63.90995788574219, 'logits/chosen': -2.786811113357544, 'logits/rejected': -2.895371198654175, 'epoch': 0.6569492917265448}\n",
      "{'loss': 0.2898, 'grad_norm': 9.041149139404297, 'learning_rate': 4.261494252873564e-05, 'rewards/chosen': -0.35056349635124207, 'rewards/rejected': -2.9917361736297607, 'rewards/accuracies': 0.8843749761581421, 'rewards/margins': 2.641172170639038, 'logps/chosen': -35.15066909790039, 'logps/rejected': -62.59073257446289, 'logits/chosen': -2.918863296508789, 'logits/rejected': -3.0843310356140137, 'epoch': 0.739067953192363}\n",
      "{'loss': 0.2833, 'grad_norm': 6.2022705078125, 'learning_rate': 4.1793924466338264e-05, 'rewards/chosen': -0.46922603249549866, 'rewards/rejected': -3.0785932540893555, 'rewards/accuracies': 0.8853124976158142, 'rewards/margins': 2.6093673706054688, 'logps/chosen': -36.34384536743164, 'logps/rejected': -63.43974304199219, 'logits/chosen': -2.76200008392334, 'logits/rejected': -2.9003052711486816, 'epoch': 0.8211866146581811}\n",
      "{'loss': 0.285, 'grad_norm': 6.250007152557373, 'learning_rate': 4.097290640394088e-05, 'rewards/chosen': -0.010727779939770699, 'rewards/rejected': -2.7281033992767334, 'rewards/accuracies': 0.8812500238418579, 'rewards/margins': 2.7173755168914795, 'logps/chosen': -31.741405487060547, 'logps/rejected': -60.022804260253906, 'logits/chosen': -2.490622043609619, 'logits/rejected': -2.645690679550171, 'epoch': 0.9033052761239991}\n",
      "{'loss': 0.276, 'grad_norm': 9.620135307312012, 'learning_rate': 4.0151888341543517e-05, 'rewards/chosen': 0.20162178575992584, 'rewards/rejected': -2.665785789489746, 'rewards/accuracies': 0.8868749737739563, 'rewards/margins': 2.8674075603485107, 'logps/chosen': -29.5786190032959, 'logps/rejected': -59.42768859863281, 'logits/chosen': -2.378058433532715, 'logits/rejected': -2.5826711654663086, 'epoch': 0.9854239375898173}\n",
      "{'loss': 0.2118, 'grad_norm': 11.75803279876709, 'learning_rate': 3.933087027914614e-05, 'rewards/chosen': 0.29076144099235535, 'rewards/rejected': -3.541881799697876, 'rewards/accuracies': 0.9194862246513367, 'rewards/margins': 3.8326427936553955, 'logps/chosen': -28.772876739501953, 'logps/rejected': -68.16197204589844, 'logits/chosen': -2.045057773590088, 'logits/rejected': -2.3627982139587402, 'epoch': 1.0673373024019708}\n",
      "{'loss': 0.1871, 'grad_norm': 12.074800491333008, 'learning_rate': 3.850985221674877e-05, 'rewards/chosen': 0.6516433954238892, 'rewards/rejected': -3.680696487426758, 'rewards/accuracies': 0.9331250190734863, 'rewards/margins': 4.332339286804199, 'logps/chosen': -25.067548751831055, 'logps/rejected': -69.50785064697266, 'logits/chosen': -1.6876775026321411, 'logits/rejected': -1.9494558572769165, 'epoch': 1.149455963867789}\n",
      "{'loss': 0.2003, 'grad_norm': 3.0433034896850586, 'learning_rate': 3.7688834154351396e-05, 'rewards/chosen': 0.8274685740470886, 'rewards/rejected': -3.3686716556549072, 'rewards/accuracies': 0.9293749928474426, 'rewards/margins': 4.196139335632324, 'logps/chosen': -23.393827438354492, 'logps/rejected': -66.43843841552734, 'logits/chosen': -1.6468720436096191, 'logits/rejected': -1.8677699565887451, 'epoch': 1.231574625333607}\n",
      "{'loss': 0.2012, 'grad_norm': 7.611546516418457, 'learning_rate': 3.686781609195403e-05, 'rewards/chosen': 1.3675678968429565, 'rewards/rejected': -2.7675247192382812, 'rewards/accuracies': 0.9243749976158142, 'rewards/margins': 4.135092258453369, 'logps/chosen': -17.932174682617188, 'logps/rejected': -60.35152816772461, 'logits/chosen': -1.3570070266723633, 'logits/rejected': -1.5884991884231567, 'epoch': 1.313693286799425}\n",
      "{'loss': 0.1813, 'grad_norm': 6.779775619506836, 'learning_rate': 3.604679802955665e-05, 'rewards/chosen': 0.9388630390167236, 'rewards/rejected': -3.2562801837921143, 'rewards/accuracies': 0.9359375238418579, 'rewards/margins': 4.195143699645996, 'logps/chosen': -22.202960968017578, 'logps/rejected': -65.25212097167969, 'logits/chosen': -1.7924392223358154, 'logits/rejected': -2.1837289333343506, 'epoch': 1.3958119482652434}\n",
      "{'loss': 0.1949, 'grad_norm': 13.538965225219727, 'learning_rate': 3.5225779967159275e-05, 'rewards/chosen': 1.2781178951263428, 'rewards/rejected': -2.7435173988342285, 'rewards/accuracies': 0.9225000143051147, 'rewards/margins': 4.021635055541992, 'logps/chosen': -18.847190856933594, 'logps/rejected': -60.14421844482422, 'logits/chosen': -1.8177119493484497, 'logits/rejected': -2.240196466445923, 'epoch': 1.4779306097310614}\n",
      "{'loss': 0.1884, 'grad_norm': 20.541399002075195, 'learning_rate': 3.440476190476191e-05, 'rewards/chosen': 0.9251096844673157, 'rewards/rejected': -3.2338638305664062, 'rewards/accuracies': 0.9334375262260437, 'rewards/margins': 4.158973217010498, 'logps/chosen': -22.357860565185547, 'logps/rejected': -64.99034881591797, 'logits/chosen': -1.2906893491744995, 'logits/rejected': -1.544517159461975, 'epoch': 1.5600492711968794}\n",
      "{'loss': 0.1791, 'grad_norm': 24.280513763427734, 'learning_rate': 3.3583743842364535e-05, 'rewards/chosen': 0.7866101264953613, 'rewards/rejected': -3.4287631511688232, 'rewards/accuracies': 0.9306250214576721, 'rewards/margins': 4.2153730392456055, 'logps/chosen': -23.686721801757812, 'logps/rejected': -66.96612548828125, 'logits/chosen': -1.3675334453582764, 'logits/rejected': -1.6335394382476807, 'epoch': 1.6421679326626974}\n",
      "{'loss': 0.186, 'grad_norm': 29.447423934936523, 'learning_rate': 3.276272577996716e-05, 'rewards/chosen': 0.8274693489074707, 'rewards/rejected': -3.3127188682556152, 'rewards/accuracies': 0.9334375262260437, 'rewards/margins': 4.140188217163086, 'logps/chosen': -23.342147827148438, 'logps/rejected': -65.80789184570312, 'logits/chosen': -1.1434600353240967, 'logits/rejected': -1.3777412176132202, 'epoch': 1.7242865941285157}\n",
      "{'loss': 0.1879, 'grad_norm': 22.11334800720215, 'learning_rate': 3.194170771756979e-05, 'rewards/chosen': 1.005789875984192, 'rewards/rejected': -3.2140469551086426, 'rewards/accuracies': 0.9331250190734863, 'rewards/margins': 4.219836711883545, 'logps/chosen': -21.532453536987305, 'logps/rejected': -64.83948516845703, 'logits/chosen': -1.1197748184204102, 'logits/rejected': -1.3676886558532715, 'epoch': 1.806405255594334}\n",
      "{'loss': 0.1674, 'grad_norm': 2.325915575027466, 'learning_rate': 3.1120689655172414e-05, 'rewards/chosen': 0.9229709506034851, 'rewards/rejected': -3.3769760131835938, 'rewards/accuracies': 0.9399999976158142, 'rewards/margins': 4.2999467849731445, 'logps/chosen': -22.38800621032715, 'logps/rejected': -66.51951599121094, 'logits/chosen': -1.1730228662490845, 'logits/rejected': -1.415401816368103, 'epoch': 1.888523917060152}\n",
      "{'loss': 0.1657, 'grad_norm': 4.966649532318115, 'learning_rate': 3.029967159277504e-05, 'rewards/chosen': 1.3435711860656738, 'rewards/rejected': -3.4364354610443115, 'rewards/accuracies': 0.9418749809265137, 'rewards/margins': 4.780006408691406, 'logps/chosen': -18.24329376220703, 'logps/rejected': -67.06617736816406, 'logits/chosen': -1.1593742370605469, 'logits/rejected': -1.4072632789611816, 'epoch': 1.97064257852597}\n",
      "{'loss': 0.1188, 'grad_norm': 10.807650566101074, 'learning_rate': 2.947865353037767e-05, 'rewards/chosen': 1.2672725915908813, 'rewards/rejected': -3.9846127033233643, 'rewards/accuracies': 0.9605262875556946, 'rewards/margins': 5.251885414123535, 'logps/chosen': -18.95528221130371, 'logps/rejected': -72.53482055664062, 'logits/chosen': -1.1118062734603882, 'logits/rejected': -1.366551399230957, 'epoch': 2.0525559433381235}\n",
      "{'loss': 0.0944, 'grad_norm': 2.7015366554260254, 'learning_rate': 2.86576354679803e-05, 'rewards/chosen': 1.4157915115356445, 'rewards/rejected': -4.777761459350586, 'rewards/accuracies': 0.9706249833106995, 'rewards/margins': 6.193553447723389, 'logps/chosen': -17.516389846801758, 'logps/rejected': -80.5165786743164, 'logits/chosen': -0.8791452050209045, 'logits/rejected': -1.1363943815231323, 'epoch': 2.1346746048039416}\n",
      "{'loss': 0.0993, 'grad_norm': 0.3053494095802307, 'learning_rate': 2.7836617405582926e-05, 'rewards/chosen': 2.049855947494507, 'rewards/rejected': -4.810704231262207, 'rewards/accuracies': 0.9746875166893005, 'rewards/margins': 6.860559463500977, 'logps/chosen': -11.136768341064453, 'logps/rejected': -80.81640625, 'logits/chosen': -0.718778669834137, 'logits/rejected': -1.173756718635559, 'epoch': 2.21679326626976}\n",
      "{'loss': 0.1025, 'grad_norm': 0.21362310647964478, 'learning_rate': 2.701559934318555e-05, 'rewards/chosen': 1.8941824436187744, 'rewards/rejected': -4.747699737548828, 'rewards/accuracies': 0.9684374928474426, 'rewards/margins': 6.641881942749023, 'logps/chosen': -12.6554594039917, 'logps/rejected': -80.18701934814453, 'logits/chosen': -0.8714027404785156, 'logits/rejected': -1.322300672531128, 'epoch': 2.298911927735578}\n",
      "{'loss': 0.0947, 'grad_norm': 33.98943328857422, 'learning_rate': 2.6194581280788176e-05, 'rewards/chosen': 1.711007833480835, 'rewards/rejected': -4.980112552642822, 'rewards/accuracies': 0.9709374904632568, 'rewards/margins': 6.691120624542236, 'logps/chosen': -14.47811508178711, 'logps/rejected': -82.51495361328125, 'logits/chosen': -1.0217618942260742, 'logits/rejected': -1.4855669736862183, 'epoch': 2.381030589201396}\n",
      "{'loss': 0.0881, 'grad_norm': 8.469127655029297, 'learning_rate': 2.5373563218390806e-05, 'rewards/chosen': 1.9492346048355103, 'rewards/rejected': -4.853938579559326, 'rewards/accuracies': 0.9756249785423279, 'rewards/margins': 6.803172588348389, 'logps/chosen': -12.142586708068848, 'logps/rejected': -81.26307678222656, 'logits/chosen': -1.433382272720337, 'logits/rejected': -2.06727933883667, 'epoch': 2.463149250667214}\n",
      "{'loss': 0.1076, 'grad_norm': 0.13800059258937836, 'learning_rate': 2.4552545155993435e-05, 'rewards/chosen': 1.8561584949493408, 'rewards/rejected': -4.498401641845703, 'rewards/accuracies': 0.9696875214576721, 'rewards/margins': 6.354559421539307, 'logps/chosen': -13.035057067871094, 'logps/rejected': -77.73770141601562, 'logits/chosen': -1.3791182041168213, 'logits/rejected': -1.9808605909347534, 'epoch': 2.545267912133032}\n",
      "{'loss': 0.104, 'grad_norm': 0.09302656352519989, 'learning_rate': 2.373152709359606e-05, 'rewards/chosen': 1.6561651229858398, 'rewards/rejected': -4.962214469909668, 'rewards/accuracies': 0.9712499976158142, 'rewards/margins': 6.61837911605835, 'logps/chosen': -15.030230522155762, 'logps/rejected': -82.36083984375, 'logits/chosen': -1.2756668329238892, 'logits/rejected': -1.8798578977584839, 'epoch': 2.62738657359885}\n",
      "{'loss': 0.0816, 'grad_norm': 1.0459568500518799, 'learning_rate': 2.2910509031198688e-05, 'rewards/chosen': 1.7546600103378296, 'rewards/rejected': -4.937272548675537, 'rewards/accuracies': 0.9768750071525574, 'rewards/margins': 6.691932201385498, 'logps/chosen': -14.112317085266113, 'logps/rejected': -82.07042694091797, 'logits/chosen': -1.2667754888534546, 'logits/rejected': -1.8448567390441895, 'epoch': 2.7095052350646682}\n",
      "{'loss': 0.108, 'grad_norm': 22.78402328491211, 'learning_rate': 2.2089490968801315e-05, 'rewards/chosen': 1.7896523475646973, 'rewards/rejected': -5.0479936599731445, 'rewards/accuracies': 0.9709374904632568, 'rewards/margins': 6.837646484375, 'logps/chosen': -13.748690605163574, 'logps/rejected': -83.13418579101562, 'logits/chosen': -1.1713166236877441, 'logits/rejected': -1.8263019323349, 'epoch': 2.7916238965304867}\n",
      "{'loss': 0.0852, 'grad_norm': 0.41757380962371826, 'learning_rate': 2.126847290640394e-05, 'rewards/chosen': 1.870937466621399, 'rewards/rejected': -4.743363857269287, 'rewards/accuracies': 0.9721875190734863, 'rewards/margins': 6.6143012046813965, 'logps/chosen': -12.888580322265625, 'logps/rejected': -80.08489227294922, 'logits/chosen': -1.2999004125595093, 'logits/rejected': -2.0273282527923584, 'epoch': 2.8737425579963047}\n",
      "{'loss': 0.0897, 'grad_norm': 1.0725213289260864, 'learning_rate': 2.044745484400657e-05, 'rewards/chosen': 2.1596121788024902, 'rewards/rejected': -5.529467582702637, 'rewards/accuracies': 0.973437488079071, 'rewards/margins': 7.689079761505127, 'logps/chosen': -9.992501258850098, 'logps/rejected': -87.90473937988281, 'logits/chosen': -1.1652544736862183, 'logits/rejected': -2.167935848236084, 'epoch': 2.9558612194621228}\n",
      "{'loss': 0.0616, 'grad_norm': 2.0334930419921875, 'learning_rate': 1.9626436781609197e-05, 'rewards/chosen': 2.1653897762298584, 'rewards/rejected': -5.300354957580566, 'rewards/accuracies': 0.9833959937095642, 'rewards/margins': 7.4657440185546875, 'logps/chosen': -9.979875564575195, 'logps/rejected': -85.68994140625, 'logits/chosen': -1.0473939180374146, 'logits/rejected': -2.027860403060913, 'epoch': 3.0377745842742763}\n",
      "{'loss': 0.039, 'grad_norm': 0.9050827026367188, 'learning_rate': 1.8805418719211824e-05, 'rewards/chosen': 2.1233596801757812, 'rewards/rejected': -6.034857273101807, 'rewards/accuracies': 0.9915624856948853, 'rewards/margins': 8.158217430114746, 'logps/chosen': -10.38525104522705, 'logps/rejected': -93.09923553466797, 'logits/chosen': -1.0941835641860962, 'logits/rejected': -1.9733855724334717, 'epoch': 3.1198932457400943}\n",
      "{'loss': 0.0424, 'grad_norm': 2.030822992324829, 'learning_rate': 1.798440065681445e-05, 'rewards/chosen': 2.1569347381591797, 'rewards/rejected': -6.0652923583984375, 'rewards/accuracies': 0.9906250238418579, 'rewards/margins': 8.222227096557617, 'logps/chosen': -10.081496238708496, 'logps/rejected': -93.33440399169922, 'logits/chosen': -1.2534797191619873, 'logits/rejected': -2.162041664123535, 'epoch': 3.2020119072059128}\n",
      "{'loss': 0.0451, 'grad_norm': 0.2510717511177063, 'learning_rate': 1.7163382594417077e-05, 'rewards/chosen': 2.1134238243103027, 'rewards/rejected': -6.105375289916992, 'rewards/accuracies': 0.9884374737739563, 'rewards/margins': 8.218799591064453, 'logps/chosen': -10.478925704956055, 'logps/rejected': -93.81554412841797, 'logits/chosen': -1.3661596775054932, 'logits/rejected': -2.3101389408111572, 'epoch': 3.284130568671731}\n",
      "{'loss': 0.0407, 'grad_norm': 5.295600414276123, 'learning_rate': 1.6342364532019706e-05, 'rewards/chosen': 2.1342251300811768, 'rewards/rejected': -6.187054634094238, 'rewards/accuracies': 0.9906250238418579, 'rewards/margins': 8.321280479431152, 'logps/chosen': -10.261577606201172, 'logps/rejected': -94.53390502929688, 'logits/chosen': -1.347892165184021, 'logits/rejected': -2.420703172683716, 'epoch': 3.366249230137549}\n",
      "{'loss': 0.0475, 'grad_norm': 4.676507472991943, 'learning_rate': 1.5521346469622333e-05, 'rewards/chosen': 2.1377713680267334, 'rewards/rejected': -6.227222442626953, 'rewards/accuracies': 0.9884374737739563, 'rewards/margins': 8.364994049072266, 'logps/chosen': -10.246410369873047, 'logps/rejected': -94.99655151367188, 'logits/chosen': -1.3100358247756958, 'logits/rejected': -2.267188787460327, 'epoch': 3.448367891603367}\n",
      "{'loss': 0.0369, 'grad_norm': 0.010027226991951466, 'learning_rate': 1.470032840722496e-05, 'rewards/chosen': 2.071279287338257, 'rewards/rejected': -6.8182148933410645, 'rewards/accuracies': 0.9915624856948853, 'rewards/margins': 8.889493942260742, 'logps/chosen': -10.885324478149414, 'logps/rejected': -100.86796569824219, 'logits/chosen': -1.3731764554977417, 'logits/rejected': -2.5564634799957275, 'epoch': 3.530486553069185}\n",
      "{'loss': 0.0328, 'grad_norm': 0.011031845584511757, 'learning_rate': 1.3879310344827587e-05, 'rewards/chosen': 2.3010098934173584, 'rewards/rejected': -6.836740493774414, 'rewards/accuracies': 0.9934375286102295, 'rewards/margins': 9.137750625610352, 'logps/chosen': -8.605401992797852, 'logps/rejected': -101.05333709716797, 'logits/chosen': -1.3092907667160034, 'logits/rejected': -2.7025527954101562, 'epoch': 3.612605214535003}\n",
      "{'loss': 0.0439, 'grad_norm': 0.2632136642932892, 'learning_rate': 1.3058292282430215e-05, 'rewards/chosen': 2.098031520843506, 'rewards/rejected': -6.529183387756348, 'rewards/accuracies': 0.9896875023841858, 'rewards/margins': 8.627213478088379, 'logps/chosen': -10.631721496582031, 'logps/rejected': -97.9835433959961, 'logits/chosen': -1.3731838464736938, 'logits/rejected': -2.5363521575927734, 'epoch': 3.694723876000821}\n",
      "{'loss': 0.0286, 'grad_norm': 0.028800662606954575, 'learning_rate': 1.2237274220032842e-05, 'rewards/chosen': 2.2451436519622803, 'rewards/rejected': -6.8088812828063965, 'rewards/accuracies': 0.9931250214576721, 'rewards/margins': 9.054024696350098, 'logps/chosen': -9.216432571411133, 'logps/rejected': -100.82131958007812, 'logits/chosen': -1.4137628078460693, 'logits/rejected': -2.6942451000213623, 'epoch': 3.7768425374666394}\n",
      "{'loss': 0.0483, 'grad_norm': 0.09607218950986862, 'learning_rate': 1.1416256157635468e-05, 'rewards/chosen': 2.2317724227905273, 'rewards/rejected': -6.482202053070068, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 8.713973999023438, 'logps/chosen': -9.288814544677734, 'logps/rejected': -97.48130798339844, 'logits/chosen': -1.4636861085891724, 'logits/rejected': -2.751896858215332, 'epoch': 3.8589611989324575}\n",
      "{'loss': 0.0421, 'grad_norm': 0.03302475064992905, 'learning_rate': 1.0595238095238096e-05, 'rewards/chosen': 2.2403931617736816, 'rewards/rejected': -6.412713527679443, 'rewards/accuracies': 0.989062488079071, 'rewards/margins': 8.653106689453125, 'logps/chosen': -9.217787742614746, 'logps/rejected': -96.8369369506836, 'logits/chosen': -1.5324816703796387, 'logits/rejected': -2.8427748680114746, 'epoch': 3.9410798603982755}\n",
      "{'loss': 0.0274, 'grad_norm': 0.11216853559017181, 'learning_rate': 9.774220032840723e-06, 'rewards/chosen': 2.1328012943267822, 'rewards/rejected': -6.983326435089111, 'rewards/accuracies': 0.9934210777282715, 'rewards/margins': 9.116127014160156, 'logps/chosen': -10.242997169494629, 'logps/rejected': -102.51541137695312, 'logits/chosen': -1.5411237478256226, 'logits/rejected': -2.9152190685272217, 'epoch': 4.0229932252104295}\n",
      "{'loss': 0.0204, 'grad_norm': 0.05577357858419418, 'learning_rate': 8.953201970443351e-06, 'rewards/chosen': 2.067183494567871, 'rewards/rejected': -6.905545711517334, 'rewards/accuracies': 0.9962499737739563, 'rewards/margins': 8.972728729248047, 'logps/chosen': -11.01300048828125, 'logps/rejected': -101.72921752929688, 'logits/chosen': -1.62986421585083, 'logits/rejected': -3.0268187522888184, 'epoch': 4.105111886676247}\n",
      "{'loss': 0.0201, 'grad_norm': 20.316110610961914, 'learning_rate': 8.132183908045977e-06, 'rewards/chosen': 1.9011030197143555, 'rewards/rejected': -7.324818134307861, 'rewards/accuracies': 0.9965624809265137, 'rewards/margins': 9.225922584533691, 'logps/chosen': -12.60737419128418, 'logps/rejected': -105.9360580444336, 'logits/chosen': -1.8613592386245728, 'logits/rejected': -3.2824032306671143, 'epoch': 4.1872305481420655}\n",
      "{'loss': 0.015, 'grad_norm': 0.06454659253358841, 'learning_rate': 7.3111658456486045e-06, 'rewards/chosen': 2.0497710704803467, 'rewards/rejected': -7.388652324676514, 'rewards/accuracies': 0.995312511920929, 'rewards/margins': 9.438423156738281, 'logps/chosen': -11.104643821716309, 'logps/rejected': -106.52499389648438, 'logits/chosen': -1.8399755954742432, 'logits/rejected': -3.3121721744537354, 'epoch': 4.269349209607883}\n",
      "{'loss': 0.0165, 'grad_norm': 0.012205091305077076, 'learning_rate': 6.490147783251233e-06, 'rewards/chosen': 2.220339775085449, 'rewards/rejected': -7.514338493347168, 'rewards/accuracies': 0.9965624809265137, 'rewards/margins': 9.7346773147583, 'logps/chosen': -9.397445678710938, 'logps/rejected': -107.84724426269531, 'logits/chosen': -1.7101572751998901, 'logits/rejected': -3.3036398887634277, 'epoch': 4.351467871073702}\n",
      "{'loss': 0.0232, 'grad_norm': 0.023213166743516922, 'learning_rate': 5.669129720853859e-06, 'rewards/chosen': 2.1516218185424805, 'rewards/rejected': -7.349801540374756, 'rewards/accuracies': 0.995312511920929, 'rewards/margins': 9.501423835754395, 'logps/chosen': -10.134016990661621, 'logps/rejected': -106.18296813964844, 'logits/chosen': -1.70027756690979, 'logits/rejected': -3.317237615585327, 'epoch': 4.43358653253952}\n",
      "{'loss': 0.0173, 'grad_norm': 0.01880158856511116, 'learning_rate': 4.848111658456486e-06, 'rewards/chosen': 2.238516092300415, 'rewards/rejected': -7.402231216430664, 'rewards/accuracies': 0.995312511920929, 'rewards/margins': 9.6407470703125, 'logps/chosen': -9.231815338134766, 'logps/rejected': -106.74102783203125, 'logits/chosen': -1.6894346475601196, 'logits/rejected': -3.4236679077148438, 'epoch': 4.515705194005338}\n",
      "{'loss': 0.0125, 'grad_norm': 0.055693600326776505, 'learning_rate': 4.027093596059114e-06, 'rewards/chosen': 2.252382278442383, 'rewards/rejected': -7.647217273712158, 'rewards/accuracies': 0.9975000023841858, 'rewards/margins': 9.899599075317383, 'logps/chosen': -9.147039413452148, 'logps/rejected': -109.2544937133789, 'logits/chosen': -1.688448190689087, 'logits/rejected': -3.449913263320923, 'epoch': 4.597823855471156}\n",
      "{'loss': 0.0145, 'grad_norm': 0.022236237302422523, 'learning_rate': 3.2060755336617404e-06, 'rewards/chosen': 2.2984731197357178, 'rewards/rejected': -7.638937950134277, 'rewards/accuracies': 0.9962499737739563, 'rewards/margins': 9.937411308288574, 'logps/chosen': -8.610830307006836, 'logps/rejected': -109.09754180908203, 'logits/chosen': -1.6765304803848267, 'logits/rejected': -3.489750862121582, 'epoch': 4.679942516936974}\n",
      "{'loss': 0.0232, 'grad_norm': 0.016719862818717957, 'learning_rate': 2.385057471264368e-06, 'rewards/chosen': 2.2927584648132324, 'rewards/rejected': -7.639163017272949, 'rewards/accuracies': 0.9946874976158142, 'rewards/margins': 9.93192195892334, 'logps/chosen': -8.646663665771484, 'logps/rejected': -109.1198501586914, 'logits/chosen': -1.7196687459945679, 'logits/rejected': -3.524113416671753, 'epoch': 4.762061178402792}\n",
      "{'loss': 0.0136, 'grad_norm': 0.03056364692747593, 'learning_rate': 1.5640394088669952e-06, 'rewards/chosen': 2.26263689994812, 'rewards/rejected': -7.673891544342041, 'rewards/accuracies': 0.9965624809265137, 'rewards/margins': 9.936528205871582, 'logps/chosen': -8.969071388244629, 'logps/rejected': -109.47673034667969, 'logits/chosen': -1.7740932703018188, 'logits/rejected': -3.555222272872925, 'epoch': 4.84417983986861}\n",
      "{'loss': 0.0252, 'grad_norm': 0.03686414286494255, 'learning_rate': 7.430213464696223e-07, 'rewards/chosen': 2.2202744483947754, 'rewards/rejected': -7.6549391746521, 'rewards/accuracies': 0.9946874976158142, 'rewards/margins': 9.875213623046875, 'logps/chosen': -9.424487113952637, 'logps/rejected': -109.23640441894531, 'logits/chosen': -1.778254747390747, 'logits/rejected': -3.5608081817626953, 'epoch': 4.926298501334428}\n",
      "{'train_runtime': 2691.6191, 'train_samples_per_second': 72.38, 'train_steps_per_second': 4.525, 'train_loss': 0.1325370284528372, 'epoch': 5.0}\n",
      "done.\n",
      "Running benchmark for Qwen2.5-1.5B-Instruct\n",
      "Running pipeline: baseline...\n",
      "done.\n",
      "Running pipeline: few_shot_sweep...\n",
      "Running configuration 1...\n",
      "Running configuration 2...\n",
      "Running configuration 3...\n",
      "Running configuration 4...\n",
      "Running configuration 5...\n",
      "done.\n",
      "Running pipeline: dpo_lora...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38964/38964 [00:00<00:00, 52268.51 examples/s]\n",
      "Extracting prompt in train dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38964/38964 [00:01<00:00, 34327.79 examples/s]\n",
      "Applying chat template to train dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38964/38964 [00:00<00:00, 40113.29 examples/s]\n",
      "Tokenizing train dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38964/38964 [00:08<00:00, 4586.08 examples/s]\n",
      "Train dataset reference log probs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [27:14<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3183, 'grad_norm': 2.4701852798461914, 'learning_rate': 1.945539135194308e-05, 'rewards/chosen': -0.8776431083679199, 'rewards/rejected': -2.9141292572021484, 'rewards/accuracies': 0.8884375095367432, 'rewards/margins': 2.0364861488342285, 'logps/chosen': -46.047210693359375, 'logps/rejected': -70.37177276611328, 'logits/chosen': -2.2215988636016846, 'logits/rejected': -2.2087676525115967, 'epoch': 0.0821186614658181}\n",
      "{'loss': 0.2653, 'grad_norm': 7.1405205726623535, 'learning_rate': 1.8908045977011497e-05, 'rewards/chosen': -0.7136953473091125, 'rewards/rejected': -3.4527807235717773, 'rewards/accuracies': 0.8934375047683716, 'rewards/margins': 2.7390854358673096, 'logps/chosen': -44.42201232910156, 'logps/rejected': -75.73184204101562, 'logits/chosen': -2.2573461532592773, 'logits/rejected': -2.3170762062072754, 'epoch': 0.1642373229316362}\n",
      "{'loss': 0.2384, 'grad_norm': 5.1630778312683105, 'learning_rate': 1.8360700602079915e-05, 'rewards/chosen': -0.4649747610092163, 'rewards/rejected': -3.737053632736206, 'rewards/accuracies': 0.9049999713897705, 'rewards/margins': 3.2720787525177, 'logps/chosen': -42.01034164428711, 'logps/rejected': -78.70819091796875, 'logits/chosen': -1.8636441230773926, 'logits/rejected': -1.986069679260254, 'epoch': 0.24635598439745432}\n",
      "{'loss': 0.223, 'grad_norm': 6.254595756530762, 'learning_rate': 1.7813355227148332e-05, 'rewards/chosen': -0.668810248374939, 'rewards/rejected': -4.2534356117248535, 'rewards/accuracies': 0.9140625, 'rewards/margins': 3.584625482559204, 'logps/chosen': -43.9538688659668, 'logps/rejected': -83.82011413574219, 'logits/chosen': -1.21739661693573, 'logits/rejected': -1.5237367153167725, 'epoch': 0.3284746458632724}\n",
      "{'loss': 0.2198, 'grad_norm': 6.308777809143066, 'learning_rate': 1.726600985221675e-05, 'rewards/chosen': -0.7813184261322021, 'rewards/rejected': -4.452877044677734, 'rewards/accuracies': 0.9143750071525574, 'rewards/margins': 3.6715590953826904, 'logps/chosen': -45.05381774902344, 'logps/rejected': -85.83983612060547, 'logits/chosen': -1.5767208337783813, 'logits/rejected': -1.8157325983047485, 'epoch': 0.41059330732909055}\n",
      "{'loss': 0.2179, 'grad_norm': 6.705678939819336, 'learning_rate': 1.6718664477285168e-05, 'rewards/chosen': -0.6038668751716614, 'rewards/rejected': -4.279495716094971, 'rewards/accuracies': 0.9131249785423279, 'rewards/margins': 3.675628662109375, 'logps/chosen': -43.277915954589844, 'logps/rejected': -83.93649291992188, 'logits/chosen': -1.564194917678833, 'logits/rejected': -1.8266748189926147, 'epoch': 0.49271196879490864}\n",
      "{'loss': 0.2165, 'grad_norm': 1.5614120960235596, 'learning_rate': 1.6171319102353585e-05, 'rewards/chosen': -0.8327794075012207, 'rewards/rejected': -4.4788737297058105, 'rewards/accuracies': 0.9165624976158142, 'rewards/margins': 3.6460940837860107, 'logps/chosen': -45.603553771972656, 'logps/rejected': -85.91009521484375, 'logits/chosen': -1.5016708374023438, 'logits/rejected': -1.8214192390441895, 'epoch': 0.5748306302607268}\n",
      "{'loss': 0.2018, 'grad_norm': 3.4801204204559326, 'learning_rate': 1.5623973727422003e-05, 'rewards/chosen': -0.45170727372169495, 'rewards/rejected': -4.376823902130127, 'rewards/accuracies': 0.9203125238418579, 'rewards/margins': 3.9251163005828857, 'logps/chosen': -41.7337532043457, 'logps/rejected': -84.97522735595703, 'logits/chosen': -1.3748949766159058, 'logits/rejected': -1.7289718389511108, 'epoch': 0.6569492917265448}\n",
      "{'loss': 0.2116, 'grad_norm': 3.675088882446289, 'learning_rate': 1.5076628352490424e-05, 'rewards/chosen': -0.39087286591529846, 'rewards/rejected': -4.3052592277526855, 'rewards/accuracies': 0.918749988079071, 'rewards/margins': 3.9143869876861572, 'logps/chosen': -41.249691009521484, 'logps/rejected': -84.33924102783203, 'logits/chosen': -1.3908292055130005, 'logits/rejected': -1.760959267616272, 'epoch': 0.739067953192363}\n"
     ]
    }
   ],
   "source": [
    "all_profiles = {}\n",
    "\n",
    "for model_name in MODELS:\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    print(f\"Running benchmark for {short_name}\")\n",
    "\n",
    "    dpo_lora = create_dpo_control(model_name)\n",
    "\n",
    "    benchmark = Benchmark(\n",
    "        use_case=commonsense_mcqa,\n",
    "        base_model_name_or_path=model_name,\n",
    "        steering_pipelines={\n",
    "            \"baseline\": [],\n",
    "            \"few_shot_sweep\": [few_shot_spec],\n",
    "            \"dpo_lora\": [dpo_lora],\n",
    "        },\n",
    "        gen_kwargs={\"max_new_tokens\": 300, \"do_sample\": True, \"temperature\": 0.7},\n",
    "        device_map=\"auto\",\n",
    "        num_trials=5\n",
    "    )\n",
    "\n",
    "    profiles = benchmark.run()\n",
    "    all_profiles[short_name] = profiles\n",
    "\n",
    "    benchmark.export(profiles, save_dir=f\"./profiles/{short_name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb6f43-ba41-4117-a366-6580013620bc",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We now analyze the benchmark results across both models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flatten_section",
   "metadata": {},
   "source": [
    "First, we flatten the nested profiles into a single DataFrame with one row per trial (using the toolkit's utility `flatten_profiles`), then aggregate across trials to get mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86624b5e-6b67-413f-87b5-9acf241f39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for model_name, profiles in all_profiles.items():\n",
    "    df = flatten_profiles(\n",
    "        profiles,\n",
    "        metric_accessors={\n",
    "            \"accuracy\": (\"MCQAAccuracy\", \"question_mean\"),\n",
    "            \"positional_bias\": (\"MCQAPositionalBias\", \"mean\"),\n",
    "        }\n",
    "    )\n",
    "    df[\"model\"] = model_name\n",
    "    df[\"k_positive\"] = get_param_values(df, \"FewShot\", \"k_positive\")\n",
    "    dfs.append(df)\n",
    "\n",
    "runs_df = pd.concat(dfs, ignore_index=True)\n",
    "runs_df[[\"model\", \"pipeline\", \"trial_id\", \"k_positive\", \"accuracy\", \"positional_bias\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9910fbdd",
   "metadata": {},
   "source": [
    "Next, we summarize by configuration (aggregating across trials) then add the `k_positive` value for each of the few-shot rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarize_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = summarize_by_config(\n",
    "    runs_df,\n",
    "    metric_cols=[\"accuracy\", \"positional_bias\"],\n",
    "    group_cols=[\"model\", \"pipeline\", \"config_id\"]\n",
    ")\n",
    "\n",
    "k_map = runs_df.groupby([\"model\", \"pipeline\", \"config_id\"])[\"k_positive\"].first()\n",
    "summary_df[\"k_positive\"] = summary_df.apply(\n",
    "    lambda row: k_map.get((row[\"model\"], row[\"pipeline\"], row[\"config_id\"]), np.nan), axis=1\n",
    ")\n",
    "\n",
    "summary_df[[\"model\", \"pipeline\", \"k_positive\", \"n_trials\", \"accuracy_mean\", \"accuracy_std\", \"positional_bias_mean\", \"positional_bias_std\"]].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewshot_scaling_section",
   "metadata": {},
   "source": [
    "### DPO vs. FewShot\n",
    "\n",
    "We now examine how the DPO-LoRA control performs in comparison to FewShot, particularly as we scale the number of (positive) examples. Both the DPO-LoRA control and baseline (unsteered) pipelines are shown as horizontal reference lines passed in using the `compare_to_pipelines` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewshot_scaling_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_df = summary_df[summary_df[\"pipeline\"] == \"few_shot_sweep\"].copy()\n",
    "few_shot_df = few_shot_df.sort_values([\"model\", \"k_positive\"])\n",
    "\n",
    "# common axis limits\n",
    "all_accuracy = runs_df[\"accuracy\"].dropna()\n",
    "ylim_accuracy = (max(0, all_accuracy.min() - 0.1), min(1, all_accuracy.max() + 0.1))\n",
    "\n",
    "n_models = len(MODELS)\n",
    "fig = plt.figure(figsize=(5 * n_models, 4))\n",
    "gs = gridspec.GridSpec(1, n_models, wspace=0.3)\n",
    "\n",
    "for idx, model_name in enumerate(MODELS):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    ax = fig.add_subplot(gs[0, idx])\n",
    "\n",
    "    # extract data under each pipeline\n",
    "    model_swept = few_shot_df[few_shot_df[\"model\"] == short_name].copy()\n",
    "    model_baseline = summary_df[(summary_df[\"model\"] == short_name) & (summary_df[\"pipeline\"] == \"baseline\")]\n",
    "    model_dpo = summary_df[(summary_df[\"model\"] == short_name) & (summary_df[\"pipeline\"] == \"dpo_lora\")]\n",
    "\n",
    "    # individual trial data (for scatter overlay)\n",
    "    model_trials = runs_df[(runs_df[\"model\"] == short_name) & (runs_df[\"pipeline\"] == \"few_shot_sweep\")]\n",
    "    \n",
    "    plot_sensitivity(\n",
    "        swept=model_swept,\n",
    "        metric=\"accuracy\",\n",
    "        sweep_col=\"k_positive\",\n",
    "        per_trial_data=model_trials,\n",
    "        compare_to_pipelines=[\n",
    "            (\"baseline\", model_baseline),\n",
    "            (\"DPO-LoRA\", model_dpo),\n",
    "        ],\n",
    "        ax=ax,\n",
    "        metric_label=\"accuracy\",\n",
    "        sweep_label=\"k_positive\",\n",
    "        title=short_name,\n",
    "        ylim=ylim_accuracy,\n",
    "    )\n",
    "\n",
    "fig.savefig(FIGURE_DIR / \"sensitivity_accuracy.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122de1e3",
   "metadata": {},
   "source": [
    "We can see that fine-tuning (under DPO-LoRA) creates a noticeable jump in performance for the 0.5B model, and a smaller, but noticeable jump for the 1.5B model. The FewShot control does provide some small accuracy gain but only for moderate number of examples (gains diminish as the number of examples increase)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tradeoff_section",
   "metadata": {},
   "source": [
    "### Accuracy vs positional bias tradeoff\n",
    "\n",
    "We examine whether there is a tradeoff between accuracy and positional bias across methods. The FewShot configurations are colored by `k_positive`, with the baseline shown as a black X marker and DPO-LoRA as a red square. The Pareto frontier indicates configurations that are not dominated by any other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tradeoff_scatter_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common axis limits\n",
    "all_accuracy = runs_df[\"accuracy\"].dropna()\n",
    "all_bias = runs_df[\"positional_bias\"].dropna()\n",
    "xlim_tradeoff = (max(0, all_accuracy.min() - 0.05), min(1, all_accuracy.max() + 0.05))\n",
    "ylim_tradeoff = (max(0, all_bias.min() - 0.02), all_bias.max() + 0.02)\n",
    "\n",
    "n_models = len(MODELS)\n",
    "fig = plt.figure(figsize=(5 * n_models, 5))\n",
    "gs = gridspec.GridSpec(1, n_models, wspace=0.3)\n",
    "\n",
    "for idx, model_name in enumerate(MODELS):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    ax = fig.add_subplot(gs[0, idx])\n",
    "\n",
    "    model_swept = few_shot_df[few_shot_df[\"model\"] == short_name].copy()\n",
    "    model_baseline = summary_df[(summary_df[\"model\"] == short_name) & (summary_df[\"pipeline\"] == \"baseline\")]\n",
    "    model_dpo = summary_df[(summary_df[\"model\"] == short_name) & (summary_df[\"pipeline\"] == \"dpo_lora\")]\n",
    "\n",
    "    plot_tradeoff(\n",
    "        swept=model_swept,\n",
    "        x_metric=\"accuracy\",\n",
    "        y_metric=\"positional_bias\",\n",
    "        sweep_col=\"k_positive\",\n",
    "        compare_to_pipelines=[\n",
    "            (\"baseline\", model_baseline),\n",
    "            (\"DPO-LoRA\", model_dpo),\n",
    "        ],\n",
    "        ax=ax,\n",
    "        x_label=\"accuracy\",\n",
    "        y_label=\"positional bias\",\n",
    "        sweep_label=\"k_positive\",\n",
    "        title=short_name,\n",
    "        show_pareto=True,\n",
    "        maximize_x=True,\n",
    "        maximize_y=False,\n",
    "        xlim=xlim_tradeoff,\n",
    "        ylim=ylim_tradeoff,\n",
    "    )\n",
    "\n",
    "fig.savefig(FIGURE_DIR / \"tradeoff.png\", bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450a31e",
   "metadata": {},
   "source": [
    "The interaction between the number of positive examples and the trade-off between positional bias and accuracy is fairly subtle/minor. In the 0.5B model, any number of positive examples causes the positional bias to jump (with a slight increase in accuracy). Interestingly it appears that when the 1.5B model is prompted with a small number of positive examples, positional bias does increase with accuracy, but falls again as the accuracy gains disappear. Generally, the DPO-LoRA control achieves the best trade-off under both models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "### Summary table\n",
    "\n",
    "The table below summarizes all configurations ranked by accuracy for all methods/models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_order = [\"baseline\", \"FewShot (k=1)\", \"FewShot (k=5)\", \"FewShot (k=10)\", \"FewShot (k=25)\", \"FewShot (k=50)\", \"DPO-LoRA\"]\n",
    "\n",
    "summary_table = summary_df.copy()\n",
    "summary_table[\"method\"] = summary_table.apply(\n",
    "    lambda row: \"baseline\" if row[\"pipeline\"] == \"baseline\"\n",
    "    else \"DPO-LoRA\" if row[\"pipeline\"] == \"dpo_lora\"\n",
    "    else f\"FewShot (k={int(row['k_positive'])})\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "model_order = [m.split(\"/\")[-1] for m in MODELS]\n",
    "summary_table[\"model_order\"] = summary_table[\"model\"].apply(lambda m: model_order.index(m) if m in model_order else len(model_order))\n",
    "summary_table[\"method_order\"] = summary_table[\"method\"].apply(lambda m: method_order.index(m) if m in method_order else len(method_order))\n",
    "\n",
    "display_df = summary_table.sort_values([\"model_order\", \"method_order\"])[\n",
    "    [\"model\", \"method\", \"n_trials\", \"accuracy_mean\", \"accuracy_std\", \"positional_bias_mean\", \"positional_bias_std\"]\n",
    "].copy()\n",
    "display_df.columns = [\"model\", \"method\", \"trials\", \"accuracy (mean)\", \"accuracy (std)\", \"pos bias (mean)\", \"pos bias (std)\"]\n",
    "\n",
    "display_df.style.format({\n",
    "    \"accuracy (mean)\": \"{:.1%}\",\n",
    "    \"accuracy (std)\": \"{:.1%}\",\n",
    "    \"pos bias (mean)\": \"{:.3f}\",\n",
    "    \"pos bias (std)\": \"{:.3f}\",\n",
    "}).background_gradient(subset=[\"accuracy (mean)\"], cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways_section",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "This notebook compared the effectiveness of LoRA adapters with few-shot learning on a commonsense MCQA task. For the commonsense MCQA task under the models studied (`Qwen/Qwen2.5-0.5B-Instruct` and `Qwen/Qwen2.5-1.5B-Instruct`), fine-tuning significantly outperforms FewShot in the smaller model, but less so in the larger model. There does appear to be a sweet spot in the number of positive examples (accuracy-wise) for both models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
